{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d674d88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all requried libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Preprocessing and model selection libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#Tesnor flow for neural networks\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers\n",
    "\n",
    "#Classification  and accuracy scoring libraries\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "#Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "#iterative library to generate all possible combinations \n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64ea0233",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading the data\n",
    "data = pd.read_csv('tmnst DATA SET.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e35d111d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74724, 785)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the data frame\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "166eb088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>H</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74719</th>\n",
       "      <td>U</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74720</th>\n",
       "      <td>R</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74721</th>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74722</th>\n",
       "      <td>J</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74723</th>\n",
       "      <td>I</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74724 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels  1  2  3  4  5  6  7  8  9  ...  775  776  777  778  779  780  \\\n",
       "0          D  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "1          F  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "2          J  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "3          H  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "4          A  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "...      ... .. .. .. .. .. .. .. .. ..  ...  ...  ...  ...  ...  ...  ...   \n",
       "74719      U  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "74720      R  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "74721      N  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "74722      J  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "74723      I  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0   \n",
       "\n",
       "       781  782  783  784  \n",
       "0        0    0    0    0  \n",
       "1        0    0    0    0  \n",
       "2        0    0    0    0  \n",
       "3        0    0    0    0  \n",
       "4        0    0    0    0  \n",
       "...    ...  ...  ...  ...  \n",
       "74719    0    0    0    0  \n",
       "74720    0    0    0    0  \n",
       "74721    0    0    0    0  \n",
       "74722    0    0    0    0  \n",
       "74723    0    0    0    0  \n",
       "\n",
       "[74724 rows x 785 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "20aaa05d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels    object\n",
       "1          int64\n",
       "2          int64\n",
       "3          int64\n",
       "4          int64\n",
       "           ...  \n",
       "780        int64\n",
       "781        int64\n",
       "782        int64\n",
       "783        int64\n",
       "784        int64\n",
       "Length: 785, dtype: object"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Data types in the data frame\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "5b430a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(74724, 690)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing columns which are all zeros\n",
    "data = data.loc[:, (data != 0).any(axis=0)]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8065373a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please split each class into 70% train and 30% test split\n",
    "X = data.iloc[:,1:]\n",
    "y = data['labels']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "847bbea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape (52306, 689)\n",
      "X_test Shape (22418, 689)\n",
      "Y_train Shape (52306,)\n",
      "Y_test Shape (22418,)\n"
     ]
    }
   ],
   "source": [
    "print('X_train Shape',X_train.shape)\n",
    "print('X_test Shape',X_test.shape)\n",
    "print('Y_train Shape',Y_train.shape)\n",
    "print('Y_test Shape',Y_test.shape)\n",
    "\n",
    "#Normalizing the data frame\n",
    "X_train = X_train/255\n",
    "X_test = X_test/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60545a2",
   "metadata": {},
   "source": [
    "# PROBLEM 1  Neural Network Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0b443d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert Y_train from pandas DataFrame to numpy array\n",
    "Y_train_array = Y_train.to_numpy()\n",
    "Y_test_array = Y_test.to_numpy()\n",
    "# Create an instance of OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Fit and transform Y_train to one-hot encode it\n",
    "Y_train_encoded = encoder.fit_transform(Y_train_array.reshape(-1, 1)).toarray()\n",
    "Y_test_encoded = encoder.fit_transform(Y_test_array.reshape(-1, 1)).toarray()\n",
    "\n",
    "# Now you can proceed with the rest of your code as before\n",
    "Y_train_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "92bdb8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build neural network\n",
    "#Paramaters: hidden_layers = number of hidden layers\n",
    "def build_model(hidden_layers):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    for idx, neurons in enumerate(hidden_layers):\n",
    "        model.add(layers.Dense(neurons, name=f'Hidden_Layer_{idx + 1}', activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(layers.Dense(26, name='Output_Layer', activation='softmax'))  # Assuming 26 output classes\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f8e7455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define architectures to try along with their names\n",
    "architectures = [\n",
    "    ([5], \"1-5\"),\n",
    "    ([10], \"1-10\"),\n",
    "    ([20], \"1-20\"),\n",
    "    ([25], \"1-25\"),\n",
    "    ([5, 5], \"2-5-5\"),\n",
    "    ([5, 10], \"2-5-10\"),\n",
    "    ([10, 5], \"2-10-5\"),\n",
    "    ([10, 10], \"2-10-10\")\n",
    "]\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c5866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:86: UserWarning:\n",
      "\n",
      "Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1356 - loss: 2.9506 - val_accuracy: 0.4243 - val_loss: 2.0909\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4720 - loss: 1.9026 - val_accuracy: 0.6390 - val_loss: 1.4073\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6611 - loss: 1.3129 - val_accuracy: 0.7541 - val_loss: 1.0489\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7699 - loss: 0.9899 - val_accuracy: 0.8051 - val_loss: 0.8856\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8078 - loss: 0.8516 - val_accuracy: 0.8246 - val_loss: 0.8005\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8227 - loss: 0.7782 - val_accuracy: 0.8373 - val_loss: 0.7459\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8375 - loss: 0.7262 - val_accuracy: 0.8476 - val_loss: 0.7093\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8481 - loss: 0.6824 - val_accuracy: 0.8529 - val_loss: 0.6824\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8541 - loss: 0.6615 - val_accuracy: 0.8590 - val_loss: 0.6613\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8605 - loss: 0.6350 - val_accuracy: 0.8627 - val_loss: 0.6435\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8641 - loss: 0.6232 - val_accuracy: 0.8666 - val_loss: 0.6288\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8707 - loss: 0.5998 - val_accuracy: 0.8693 - val_loss: 0.6160\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8725 - loss: 0.5982 - val_accuracy: 0.8704 - val_loss: 0.6082\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8747 - loss: 0.5848 - val_accuracy: 0.8720 - val_loss: 0.5984\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.5652 - val_accuracy: 0.8761 - val_loss: 0.5873\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8791 - loss: 0.5731 - val_accuracy: 0.8774 - val_loss: 0.5804\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8796 - loss: 0.5616 - val_accuracy: 0.8784 - val_loss: 0.5739\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8822 - loss: 0.5489 - val_accuracy: 0.8807 - val_loss: 0.5680\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8826 - loss: 0.5447 - val_accuracy: 0.8809 - val_loss: 0.5613\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8846 - loss: 0.5316 - val_accuracy: 0.8812 - val_loss: 0.5566\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8868 - loss: 0.5315 - val_accuracy: 0.8832 - val_loss: 0.5518\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8851 - loss: 0.5400 - val_accuracy: 0.8857 - val_loss: 0.5466\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8881 - loss: 0.5147 - val_accuracy: 0.8849 - val_loss: 0.5431\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8863 - loss: 0.5223 - val_accuracy: 0.8861 - val_loss: 0.5383\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8894 - loss: 0.5099 - val_accuracy: 0.8880 - val_loss: 0.5337\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8897 - loss: 0.5096 - val_accuracy: 0.8875 - val_loss: 0.5310\n",
      "Epoch 27/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8919 - loss: 0.5066 - val_accuracy: 0.8902 - val_loss: 0.5269\n",
      "Epoch 28/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8900 - loss: 0.5061 - val_accuracy: 0.8892 - val_loss: 0.5234\n",
      "Epoch 29/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8916 - loss: 0.4977 - val_accuracy: 0.8912 - val_loss: 0.5187\n",
      "Epoch 30/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8921 - loss: 0.4925 - val_accuracy: 0.8906 - val_loss: 0.5169\n",
      "Epoch 31/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8920 - loss: 0.4938 - val_accuracy: 0.8895 - val_loss: 0.5150\n",
      "Epoch 32/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8967 - loss: 0.4796 - val_accuracy: 0.8921 - val_loss: 0.5104\n",
      "Epoch 33/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8949 - loss: 0.4828 - val_accuracy: 0.8905 - val_loss: 0.5110\n",
      "Epoch 34/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8969 - loss: 0.4736 - val_accuracy: 0.8923 - val_loss: 0.5079\n",
      "Epoch 35/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8972 - loss: 0.4768 - val_accuracy: 0.8926 - val_loss: 0.5046\n",
      "Epoch 36/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8996 - loss: 0.4662 - val_accuracy: 0.8917 - val_loss: 0.5052\n",
      "Epoch 37/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8980 - loss: 0.4772 - val_accuracy: 0.8945 - val_loss: 0.5002\n",
      "Epoch 38/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9014 - loss: 0.4625 - val_accuracy: 0.8942 - val_loss: 0.4984\n",
      "Epoch 39/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8993 - loss: 0.4768 - val_accuracy: 0.8950 - val_loss: 0.4962\n",
      "Epoch 40/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8997 - loss: 0.4657 - val_accuracy: 0.8952 - val_loss: 0.4945\n",
      "Epoch 41/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9018 - loss: 0.4621 - val_accuracy: 0.8956 - val_loss: 0.4921\n",
      "Epoch 42/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9006 - loss: 0.4653 - val_accuracy: 0.8964 - val_loss: 0.4915\n",
      "Epoch 43/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9002 - loss: 0.4629 - val_accuracy: 0.8961 - val_loss: 0.4902\n",
      "Epoch 44/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9033 - loss: 0.4597 - val_accuracy: 0.8964 - val_loss: 0.4898\n",
      "Epoch 45/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9026 - loss: 0.4502 - val_accuracy: 0.8963 - val_loss: 0.4883\n",
      "Epoch 46/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9003 - loss: 0.4529 - val_accuracy: 0.8963 - val_loss: 0.4874\n",
      "Epoch 47/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9009 - loss: 0.4621 - val_accuracy: 0.8976 - val_loss: 0.4853\n",
      "Epoch 48/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9030 - loss: 0.4507 - val_accuracy: 0.8964 - val_loss: 0.4846\n",
      "Epoch 49/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9002 - loss: 0.4552 - val_accuracy: 0.8966 - val_loss: 0.4835\n",
      "Epoch 50/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9051 - loss: 0.4482 - val_accuracy: 0.8949 - val_loss: 0.4856\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 15ms/step - accuracy: 0.3007 - loss: 2.6186 - val_accuracy: 0.7986 - val_loss: 1.0187\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8287 - loss: 0.8553 - val_accuracy: 0.8714 - val_loss: 0.6027\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8791 - loss: 0.5596 - val_accuracy: 0.8912 - val_loss: 0.4916\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8932 - loss: 0.4727 - val_accuracy: 0.9014 - val_loss: 0.4382\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9061 - loss: 0.4189 - val_accuracy: 0.9087 - val_loss: 0.4091\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9116 - loss: 0.3848 - val_accuracy: 0.9130 - val_loss: 0.3893\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9176 - loss: 0.3646 - val_accuracy: 0.9155 - val_loss: 0.3722\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9214 - loss: 0.3528 - val_accuracy: 0.9198 - val_loss: 0.3606\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9244 - loss: 0.3409 - val_accuracy: 0.9229 - val_loss: 0.3519\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9253 - loss: 0.3330 - val_accuracy: 0.9230 - val_loss: 0.3452\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9294 - loss: 0.3153 - val_accuracy: 0.9251 - val_loss: 0.3402\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9293 - loss: 0.3200 - val_accuracy: 0.9246 - val_loss: 0.3367\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9299 - loss: 0.3149 - val_accuracy: 0.9274 - val_loss: 0.3300\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9312 - loss: 0.3088 - val_accuracy: 0.9284 - val_loss: 0.3260\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9339 - loss: 0.3051 - val_accuracy: 0.9290 - val_loss: 0.3226\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9350 - loss: 0.2983 - val_accuracy: 0.9290 - val_loss: 0.3207\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9349 - loss: 0.2913 - val_accuracy: 0.9292 - val_loss: 0.3185\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9357 - loss: 0.2887 - val_accuracy: 0.9305 - val_loss: 0.3153\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9386 - loss: 0.2840 - val_accuracy: 0.9301 - val_loss: 0.3153\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9369 - loss: 0.2880 - val_accuracy: 0.9312 - val_loss: 0.3114\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9374 - loss: 0.2857 - val_accuracy: 0.9313 - val_loss: 0.3115\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9375 - loss: 0.2854 - val_accuracy: 0.9320 - val_loss: 0.3091\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9388 - loss: 0.2740 - val_accuracy: 0.9320 - val_loss: 0.3061\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9396 - loss: 0.2765 - val_accuracy: 0.9318 - val_loss: 0.3059\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9412 - loss: 0.2708 - val_accuracy: 0.9334 - val_loss: 0.3036\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9398 - loss: 0.2709 - val_accuracy: 0.9331 - val_loss: 0.3055\n",
      "Epoch 27/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9419 - loss: 0.2689 - val_accuracy: 0.9339 - val_loss: 0.3037\n",
      "Epoch 28/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9410 - loss: 0.2632 - val_accuracy: 0.9341 - val_loss: 0.3024\n",
      "Epoch 29/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.2630 - val_accuracy: 0.9346 - val_loss: 0.2987\n",
      "Epoch 30/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9460 - loss: 0.2498 - val_accuracy: 0.9340 - val_loss: 0.3013\n",
      "Epoch 31/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9437 - loss: 0.2585 - val_accuracy: 0.9348 - val_loss: 0.2998\n",
      "Epoch 32/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9407 - loss: 0.2612 - val_accuracy: 0.9351 - val_loss: 0.2964\n",
      "Epoch 33/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9432 - loss: 0.2570 - val_accuracy: 0.9349 - val_loss: 0.2970\n",
      "Epoch 34/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9450 - loss: 0.2466 - val_accuracy: 0.9357 - val_loss: 0.2968\n",
      "Epoch 35/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9440 - loss: 0.2538 - val_accuracy: 0.9359 - val_loss: 0.2962\n",
      "Epoch 36/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9428 - loss: 0.2497 - val_accuracy: 0.9364 - val_loss: 0.2954\n",
      "Epoch 37/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9463 - loss: 0.2447 - val_accuracy: 0.9360 - val_loss: 0.2938\n",
      "Epoch 38/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9460 - loss: 0.2396 - val_accuracy: 0.9356 - val_loss: 0.2979\n",
      "Epoch 39/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9460 - loss: 0.2444 - val_accuracy: 0.9364 - val_loss: 0.2955\n",
      "Epoch 40/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9471 - loss: 0.2371 - val_accuracy: 0.9361 - val_loss: 0.2923\n",
      "Epoch 41/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9456 - loss: 0.2401 - val_accuracy: 0.9368 - val_loss: 0.2922\n",
      "Epoch 42/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9450 - loss: 0.2418 - val_accuracy: 0.9360 - val_loss: 0.2918\n",
      "Epoch 43/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.2382 - val_accuracy: 0.9377 - val_loss: 0.2936\n",
      "Epoch 44/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.2349 - val_accuracy: 0.9373 - val_loss: 0.2920\n",
      "Epoch 45/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9458 - loss: 0.2397 - val_accuracy: 0.9370 - val_loss: 0.2924\n",
      "Epoch 46/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9461 - loss: 0.2351 - val_accuracy: 0.9371 - val_loss: 0.2893\n",
      "Epoch 47/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9473 - loss: 0.2349 - val_accuracy: 0.9366 - val_loss: 0.2923\n",
      "Epoch 48/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9462 - loss: 0.2432 - val_accuracy: 0.9364 - val_loss: 0.2907\n",
      "Epoch 49/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9486 - loss: 0.2254 - val_accuracy: 0.9376 - val_loss: 0.2910\n",
      "Epoch 50/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9485 - loss: 0.2309 - val_accuracy: 0.9371 - val_loss: 0.2895\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.4829 - loss: 2.1671 - val_accuracy: 0.8817 - val_loss: 0.5878\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.5089 - val_accuracy: 0.9127 - val_loss: 0.4030\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9185 - loss: 0.3784 - val_accuracy: 0.9234 - val_loss: 0.3405\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9301 - loss: 0.3187 - val_accuracy: 0.9325 - val_loss: 0.3077\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9351 - loss: 0.2914 - val_accuracy: 0.9367 - val_loss: 0.2865\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9404 - loss: 0.2630 - val_accuracy: 0.9399 - val_loss: 0.2727\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9416 - loss: 0.2643 - val_accuracy: 0.9418 - val_loss: 0.2652\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9466 - loss: 0.2383 - val_accuracy: 0.9433 - val_loss: 0.2586\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9481 - loss: 0.2314 - val_accuracy: 0.9452 - val_loss: 0.2515\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9493 - loss: 0.2242 - val_accuracy: 0.9454 - val_loss: 0.2475\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9512 - loss: 0.2206 - val_accuracy: 0.9456 - val_loss: 0.2449\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9520 - loss: 0.2123 - val_accuracy: 0.9472 - val_loss: 0.2415\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9533 - loss: 0.2114 - val_accuracy: 0.9466 - val_loss: 0.2407\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9532 - loss: 0.2122 - val_accuracy: 0.9479 - val_loss: 0.2371\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9543 - loss: 0.2044 - val_accuracy: 0.9487 - val_loss: 0.2352\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9542 - loss: 0.2017 - val_accuracy: 0.9490 - val_loss: 0.2326\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9566 - loss: 0.1893 - val_accuracy: 0.9493 - val_loss: 0.2324\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9567 - loss: 0.1914 - val_accuracy: 0.9483 - val_loss: 0.2318\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9582 - loss: 0.1884 - val_accuracy: 0.9501 - val_loss: 0.2306\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9590 - loss: 0.1840 - val_accuracy: 0.9508 - val_loss: 0.2277\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1790 - val_accuracy: 0.9498 - val_loss: 0.2314\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9586 - loss: 0.1842 - val_accuracy: 0.9501 - val_loss: 0.2296\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9600 - loss: 0.1779 - val_accuracy: 0.9501 - val_loss: 0.2277\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9603 - loss: 0.1751 - val_accuracy: 0.9496 - val_loss: 0.2291\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9617 - loss: 0.1759 - val_accuracy: 0.9497 - val_loss: 0.2283\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9599 - loss: 0.1757 - val_accuracy: 0.9503 - val_loss: 0.2291\n",
      "Epoch 27/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9602 - loss: 0.1749 - val_accuracy: 0.9505 - val_loss: 0.2269\n",
      "Epoch 28/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9612 - loss: 0.1712 - val_accuracy: 0.9495 - val_loss: 0.2271\n",
      "Epoch 29/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9605 - loss: 0.1670 - val_accuracy: 0.9508 - val_loss: 0.2263\n",
      "Epoch 30/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9626 - loss: 0.1690 - val_accuracy: 0.9506 - val_loss: 0.2270\n",
      "Epoch 31/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9625 - loss: 0.1652 - val_accuracy: 0.9512 - val_loss: 0.2273\n",
      "Epoch 32/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9627 - loss: 0.1648 - val_accuracy: 0.9501 - val_loss: 0.2270\n",
      "Epoch 33/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9637 - loss: 0.1628 - val_accuracy: 0.9509 - val_loss: 0.2261\n",
      "Epoch 34/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1610 - val_accuracy: 0.9497 - val_loss: 0.2292\n",
      "Epoch 35/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9640 - loss: 0.1590 - val_accuracy: 0.9501 - val_loss: 0.2280\n",
      "Epoch 36/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9642 - loss: 0.1618 - val_accuracy: 0.9501 - val_loss: 0.2276\n",
      "Epoch 37/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9633 - loss: 0.1654 - val_accuracy: 0.9495 - val_loss: 0.2279\n",
      "Epoch 38/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.1556 - val_accuracy: 0.9495 - val_loss: 0.2296\n",
      "Epoch 39/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9642 - loss: 0.1561 - val_accuracy: 0.9501 - val_loss: 0.2283\n",
      "Epoch 40/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9655 - loss: 0.1516 - val_accuracy: 0.9509 - val_loss: 0.2266\n",
      "Epoch 41/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9664 - loss: 0.1489 - val_accuracy: 0.9503 - val_loss: 0.2301\n",
      "Epoch 42/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9658 - loss: 0.1523 - val_accuracy: 0.9501 - val_loss: 0.2288\n",
      "Epoch 43/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9649 - loss: 0.1536 - val_accuracy: 0.9503 - val_loss: 0.2291\n",
      "Epoch 44/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9653 - loss: 0.1521 - val_accuracy: 0.9497 - val_loss: 0.2300\n",
      "Epoch 45/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9650 - loss: 0.1545 - val_accuracy: 0.9490 - val_loss: 0.2309\n",
      "Epoch 46/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9657 - loss: 0.1510 - val_accuracy: 0.9499 - val_loss: 0.2316\n",
      "Epoch 47/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9683 - loss: 0.1417 - val_accuracy: 0.9508 - val_loss: 0.2290\n",
      "Epoch 48/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.9659 - loss: 0.1476 - val_accuracy: 0.9502 - val_loss: 0.2311\n",
      "Epoch 49/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.1494 - val_accuracy: 0.9497 - val_loss: 0.2324\n",
      "Epoch 50/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9671 - loss: 0.1425 - val_accuracy: 0.9500 - val_loss: 0.2323\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.5051 - loss: 2.0687 - val_accuracy: 0.8884 - val_loss: 0.5379\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9003 - loss: 0.4649 - val_accuracy: 0.9185 - val_loss: 0.3713\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9243 - loss: 0.3415 - val_accuracy: 0.9293 - val_loss: 0.3184\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9337 - loss: 0.2990 - val_accuracy: 0.9365 - val_loss: 0.2855\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9400 - loss: 0.2666 - val_accuracy: 0.9396 - val_loss: 0.2718\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9438 - loss: 0.2516 - val_accuracy: 0.9434 - val_loss: 0.2559\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9471 - loss: 0.2370 - val_accuracy: 0.9450 - val_loss: 0.2488\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9507 - loss: 0.2268 - val_accuracy: 0.9439 - val_loss: 0.2442\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9508 - loss: 0.2190 - val_accuracy: 0.9458 - val_loss: 0.2393\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9517 - loss: 0.2154 - val_accuracy: 0.9474 - val_loss: 0.2372\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9545 - loss: 0.2007 - val_accuracy: 0.9486 - val_loss: 0.2327\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9547 - loss: 0.2025 - val_accuracy: 0.9482 - val_loss: 0.2301\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9565 - loss: 0.1974 - val_accuracy: 0.9473 - val_loss: 0.2291\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.1867 - val_accuracy: 0.9492 - val_loss: 0.2275\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9593 - loss: 0.1834 - val_accuracy: 0.9491 - val_loss: 0.2237\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9591 - loss: 0.1836 - val_accuracy: 0.9496 - val_loss: 0.2235\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9600 - loss: 0.1822 - val_accuracy: 0.9498 - val_loss: 0.2219\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9603 - loss: 0.1821 - val_accuracy: 0.9500 - val_loss: 0.2235\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9606 - loss: 0.1726 - val_accuracy: 0.9495 - val_loss: 0.2223\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9611 - loss: 0.1734 - val_accuracy: 0.9499 - val_loss: 0.2210\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9619 - loss: 0.1662 - val_accuracy: 0.9500 - val_loss: 0.2210\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9632 - loss: 0.1631 - val_accuracy: 0.9504 - val_loss: 0.2197\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9655 - loss: 0.1570 - val_accuracy: 0.9503 - val_loss: 0.2195\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9644 - loss: 0.1601 - val_accuracy: 0.9505 - val_loss: 0.2206\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9653 - loss: 0.1524 - val_accuracy: 0.9512 - val_loss: 0.2177\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9645 - loss: 0.1544 - val_accuracy: 0.9515 - val_loss: 0.2194\n",
      "Epoch 27/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9659 - loss: 0.1549 - val_accuracy: 0.9519 - val_loss: 0.2186\n",
      "Epoch 28/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9663 - loss: 0.1490 - val_accuracy: 0.9510 - val_loss: 0.2222\n",
      "Epoch 29/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9678 - loss: 0.1434 - val_accuracy: 0.9511 - val_loss: 0.2185\n",
      "Epoch 30/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9666 - loss: 0.1460 - val_accuracy: 0.9517 - val_loss: 0.2191\n",
      "Epoch 31/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9668 - loss: 0.1494 - val_accuracy: 0.9517 - val_loss: 0.2197\n",
      "Epoch 32/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9676 - loss: 0.1472 - val_accuracy: 0.9516 - val_loss: 0.2177\n",
      "Epoch 33/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9680 - loss: 0.1470 - val_accuracy: 0.9526 - val_loss: 0.2181\n",
      "Epoch 34/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.1441 - val_accuracy: 0.9523 - val_loss: 0.2183\n",
      "Epoch 35/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1321 - val_accuracy: 0.9518 - val_loss: 0.2207\n",
      "Epoch 36/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9681 - loss: 0.1394 - val_accuracy: 0.9523 - val_loss: 0.2183\n",
      "Epoch 37/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.1415 - val_accuracy: 0.9518 - val_loss: 0.2222\n",
      "Epoch 38/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9685 - loss: 0.1360 - val_accuracy: 0.9521 - val_loss: 0.2209\n",
      "Epoch 39/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1309 - val_accuracy: 0.9525 - val_loss: 0.2200\n",
      "Epoch 40/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.1367 - val_accuracy: 0.9504 - val_loss: 0.2252\n",
      "Epoch 41/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9698 - loss: 0.1323 - val_accuracy: 0.9514 - val_loss: 0.2254\n",
      "Epoch 42/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9714 - loss: 0.1271 - val_accuracy: 0.9520 - val_loss: 0.2234\n",
      "Epoch 43/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9694 - loss: 0.1314 - val_accuracy: 0.9522 - val_loss: 0.2205\n",
      "Epoch 44/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9721 - loss: 0.1250 - val_accuracy: 0.9521 - val_loss: 0.2235\n",
      "Epoch 45/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9708 - loss: 0.1257 - val_accuracy: 0.9516 - val_loss: 0.2244\n",
      "Epoch 46/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9696 - loss: 0.1327 - val_accuracy: 0.9523 - val_loss: 0.2215\n",
      "Epoch 47/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9723 - loss: 0.1225 - val_accuracy: 0.9524 - val_loss: 0.2252\n",
      "Epoch 48/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9719 - loss: 0.1234 - val_accuracy: 0.9521 - val_loss: 0.2226\n",
      "Epoch 49/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9704 - loss: 0.1313 - val_accuracy: 0.9517 - val_loss: 0.2245\n",
      "Epoch 50/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9728 - loss: 0.1238 - val_accuracy: 0.9516 - val_loss: 0.2266\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0919 - loss: 3.0085 - val_accuracy: 0.3379 - val_loss: 2.1844\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.4461 - loss: 1.8894 - val_accuracy: 0.6571 - val_loss: 1.2691\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6832 - loss: 1.1444 - val_accuracy: 0.7647 - val_loss: 0.9169\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7792 - loss: 0.8621 - val_accuracy: 0.8119 - val_loss: 0.7645\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8250 - loss: 0.7213 - val_accuracy: 0.8346 - val_loss: 0.6933\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8394 - loss: 0.6784 - val_accuracy: 0.8439 - val_loss: 0.6557\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8490 - loss: 0.6331 - val_accuracy: 0.8519 - val_loss: 0.6304\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8556 - loss: 0.6185 - val_accuracy: 0.8566 - val_loss: 0.6140\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8613 - loss: 0.5935 - val_accuracy: 0.8592 - val_loss: 0.6047\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8618 - loss: 0.5909 - val_accuracy: 0.8637 - val_loss: 0.5904\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8687 - loss: 0.5597 - val_accuracy: 0.8660 - val_loss: 0.5794\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8725 - loss: 0.5535 - val_accuracy: 0.8703 - val_loss: 0.5675\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8717 - loss: 0.5535 - val_accuracy: 0.8700 - val_loss: 0.5684\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8778 - loss: 0.5396 - val_accuracy: 0.8755 - val_loss: 0.5542\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8787 - loss: 0.5271 - val_accuracy: 0.8755 - val_loss: 0.5502\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8808 - loss: 0.5157 - val_accuracy: 0.8780 - val_loss: 0.5418\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8839 - loss: 0.5142 - val_accuracy: 0.8792 - val_loss: 0.5409\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8840 - loss: 0.5137 - val_accuracy: 0.8809 - val_loss: 0.5301\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8849 - loss: 0.5003 - val_accuracy: 0.8825 - val_loss: 0.5232\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8854 - loss: 0.5060 - val_accuracy: 0.8840 - val_loss: 0.5209\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8893 - loss: 0.4947 - val_accuracy: 0.8844 - val_loss: 0.5172\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8904 - loss: 0.4879 - val_accuracy: 0.8866 - val_loss: 0.5098\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8895 - loss: 0.4819 - val_accuracy: 0.8867 - val_loss: 0.5058\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8898 - loss: 0.4950 - val_accuracy: 0.8836 - val_loss: 0.5132\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8923 - loss: 0.4694 - val_accuracy: 0.8887 - val_loss: 0.5012\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8950 - loss: 0.4600 - val_accuracy: 0.8889 - val_loss: 0.4971\n",
      "Epoch 27/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8947 - loss: 0.4694 - val_accuracy: 0.8902 - val_loss: 0.4940\n",
      "Epoch 28/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8945 - loss: 0.4674 - val_accuracy: 0.8891 - val_loss: 0.4949\n",
      "Epoch 29/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8919 - loss: 0.4696 - val_accuracy: 0.8927 - val_loss: 0.4849\n",
      "Epoch 30/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8971 - loss: 0.4555 - val_accuracy: 0.8927 - val_loss: 0.4860\n",
      "Epoch 31/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8962 - loss: 0.4582 - val_accuracy: 0.8940 - val_loss: 0.4775\n",
      "Epoch 32/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8956 - loss: 0.4588 - val_accuracy: 0.8948 - val_loss: 0.4760\n",
      "Epoch 33/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9005 - loss: 0.4425 - val_accuracy: 0.8946 - val_loss: 0.4748\n",
      "Epoch 34/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8987 - loss: 0.4464 - val_accuracy: 0.8946 - val_loss: 0.4711\n",
      "Epoch 35/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8992 - loss: 0.4345 - val_accuracy: 0.8935 - val_loss: 0.4738\n",
      "Epoch 36/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8989 - loss: 0.4476 - val_accuracy: 0.8962 - val_loss: 0.4697\n",
      "Epoch 37/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9005 - loss: 0.4449 - val_accuracy: 0.8963 - val_loss: 0.4684\n",
      "Epoch 38/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9013 - loss: 0.4357 - val_accuracy: 0.8965 - val_loss: 0.4669\n",
      "Epoch 39/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9003 - loss: 0.4401 - val_accuracy: 0.8959 - val_loss: 0.4657\n",
      "Epoch 40/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9038 - loss: 0.4282 - val_accuracy: 0.8979 - val_loss: 0.4645\n",
      "Epoch 41/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9012 - loss: 0.4337 - val_accuracy: 0.8984 - val_loss: 0.4609\n",
      "Epoch 42/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9028 - loss: 0.4349 - val_accuracy: 0.8974 - val_loss: 0.4604\n",
      "Epoch 43/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9040 - loss: 0.4226 - val_accuracy: 0.8974 - val_loss: 0.4601\n",
      "Epoch 44/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9035 - loss: 0.4235 - val_accuracy: 0.8971 - val_loss: 0.4621\n",
      "Epoch 45/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9012 - loss: 0.4292 - val_accuracy: 0.8991 - val_loss: 0.4564\n",
      "Epoch 46/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9043 - loss: 0.4248 - val_accuracy: 0.8995 - val_loss: 0.4546\n",
      "Epoch 47/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9044 - loss: 0.4197 - val_accuracy: 0.8983 - val_loss: 0.4584\n",
      "Epoch 48/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9064 - loss: 0.4158 - val_accuracy: 0.9005 - val_loss: 0.4533\n",
      "Epoch 49/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9070 - loss: 0.4159 - val_accuracy: 0.8969 - val_loss: 0.4586\n",
      "Epoch 50/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.9036 - loss: 0.4203 - val_accuracy: 0.9016 - val_loss: 0.4521\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.1416 - loss: 2.9552 - val_accuracy: 0.5253 - val_loss: 1.7488\n",
      "Epoch 2/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6213 - loss: 1.4568 - val_accuracy: 0.7549 - val_loss: 0.9847\n",
      "Epoch 3/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7827 - loss: 0.8995 - val_accuracy: 0.8142 - val_loss: 0.7775\n",
      "Epoch 4/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 0.7451 - val_accuracy: 0.8355 - val_loss: 0.6958\n",
      "Epoch 5/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8413 - loss: 0.6795 - val_accuracy: 0.8474 - val_loss: 0.6528\n",
      "Epoch 6/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8517 - loss: 0.6462 - val_accuracy: 0.8596 - val_loss: 0.6229\n",
      "Epoch 7/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8635 - loss: 0.5914 - val_accuracy: 0.8649 - val_loss: 0.6024\n",
      "Epoch 8/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8672 - loss: 0.5873 - val_accuracy: 0.8685 - val_loss: 0.5885\n",
      "Epoch 9/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8697 - loss: 0.5726 - val_accuracy: 0.8710 - val_loss: 0.5781\n",
      "Epoch 10/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8740 - loss: 0.5622 - val_accuracy: 0.8724 - val_loss: 0.5705\n",
      "Epoch 11/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8766 - loss: 0.5551 - val_accuracy: 0.8748 - val_loss: 0.5624\n",
      "Epoch 12/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8785 - loss: 0.5522 - val_accuracy: 0.8775 - val_loss: 0.5542\n",
      "Epoch 13/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8813 - loss: 0.5348 - val_accuracy: 0.8782 - val_loss: 0.5517\n",
      "Epoch 14/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8827 - loss: 0.5273 - val_accuracy: 0.8779 - val_loss: 0.5459\n",
      "Epoch 15/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8860 - loss: 0.5159 - val_accuracy: 0.8801 - val_loss: 0.5407\n",
      "Epoch 16/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8875 - loss: 0.5204 - val_accuracy: 0.8827 - val_loss: 0.5334\n",
      "Epoch 17/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8866 - loss: 0.5174 - val_accuracy: 0.8842 - val_loss: 0.5270\n",
      "Epoch 18/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8898 - loss: 0.5073 - val_accuracy: 0.8863 - val_loss: 0.5195\n",
      "Epoch 19/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8907 - loss: 0.5030 - val_accuracy: 0.8854 - val_loss: 0.5182\n",
      "Epoch 20/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.4903 - val_accuracy: 0.8869 - val_loss: 0.5154\n",
      "Epoch 21/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8945 - loss: 0.4936 - val_accuracy: 0.8878 - val_loss: 0.5087\n",
      "Epoch 22/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8957 - loss: 0.4851 - val_accuracy: 0.8907 - val_loss: 0.5033\n",
      "Epoch 23/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8944 - loss: 0.4954 - val_accuracy: 0.8900 - val_loss: 0.5023\n",
      "Epoch 24/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8959 - loss: 0.4731 - val_accuracy: 0.8908 - val_loss: 0.4994\n",
      "Epoch 25/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8988 - loss: 0.4713 - val_accuracy: 0.8922 - val_loss: 0.4908\n",
      "Epoch 26/50\n",
      "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8974 - loss: 0.4702 - val_accuracy: 0.8940 - val_loss: 0.4901\n",
      "Epoch 27/50\n",
      "\u001b[1m193/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8969 - loss: 0.4773"
     ]
    }
   ],
   "source": [
    "# Iterate over each architecture\n",
    "for architecture, name in architectures:\n",
    "    # Build the model\n",
    "    model = build_model(architecture)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Fit the model with one-hot encoded Y_train\n",
    "    history = model.fit(X_train, Y_train_encoded, epochs=50, batch_size=250, validation_data=(X_test, Y_test_encoded))\n",
    "    \n",
    "    # Compute the number of parameters\n",
    "    num_params = model.count_params()\n",
    "    \n",
    "    # Get final accuracy\n",
    "    final_accuracy = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    # Store results along with architecture name\n",
    "    results.append((name, num_params, final_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0fe27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsdf = pd.DataFrame(results,columns = ['Architecture Name', 'Parameters','Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debbdfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e9a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(modelsdf['Architecture Name'], modelsdf['Accuracy'], color='blue')\n",
    "plt.title('Architecture vs. Accuracy')\n",
    "plt.xlabel('Architecture')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e22c7a",
   "metadata": {},
   "source": [
    "Inference:\n",
    "In the above code, different neural network architectures were created with varying numbers of layers and neurons, all using the same activation function (ReLU, Sigmoid(output)), optimizer (Adam), and loss function (Categorical Crossentropy). Each model was trained for 50 epochs with a batch size of 250.\n",
    "\n",
    "Upon plotting a scatter plot of the accuracies achieved by these models, it was observed that as the number of neurons increased, the model accuracies also increased. Initially, when increasing the number of hidden layers, there was a slight drop in accuracy, but as the number of neurons in these layers increased, the accuracy also increased.\n",
    "\n",
    "From this analysis, it can be inferred that increasing the number of neurons aids in improving the model's accuracy by allowing it to capture more complex patterns in the data. Additionally, increasing the depth of the network (i.e., adding more hidden layers) can lead to better performance, provided that an adequate number of neurons are present in these layers to effectively learn from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e2768",
   "metadata": {},
   "source": [
    "# PROBLEM 2  SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b632a5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load and get the accuraices of linear SVM model\n",
    "#Parameters: X_train, Y_train, X_test, Y_test : split data, c = hyperparameter\n",
    "def svm_linear(X_train, X_test, Y_train, Y_test,c):\n",
    "    svm = SVC(kernel = 'linear', C=c)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    num_vectors = len(svm.support_vectors_)\n",
    "    train_accuracy = accuracy_score(Y_train, svm.predict(X_train))\n",
    "    test_accuracy = accuracy_score(Y_test, svm.predict(X_test))\n",
    "    return num_vectors, train_accuracy, test_accuracy\n",
    "\n",
    "#Function to load and get the accuraices of Polynomial SVM model\n",
    "#Parameters: X_train, Y_train, X_test, Y_test : split data, c = hyperparameter\n",
    "def svm_polynomial(X_train, X_test, Y_train, Y_test, d,  c):\n",
    "    svm = SVC(kernel ='poly',degree = d, C=c)\n",
    "    svm.fit(X_train, Y_train)\n",
    "    num_vectors = len(svm.support_vectors_)\n",
    "    train_accuracy = accuracy_score(Y_train, svm.predict(X_train))\n",
    "    test_accuracy = accuracy_score(Y_test, svm.predict(X_test))\n",
    "    return num_vectors, train_accuracy, test_accuracy\n",
    "\n",
    "#Function to load and get the accuraices of rbf SVM model\n",
    "#Parameters: X_train, Y_train, X_test, Y_test : split data, c = hyperparameter\n",
    "def svm_rbf(X_train, X_test, Y_train, Y_test, sigma):\n",
    "    svm = SVC(kernel = 'rbf',gamma = 1/(2*sigma**2))\n",
    "    svm.fit(X_train, Y_train)\n",
    "    num_vectors = len(svm.support_vectors_)\n",
    "    train_accuracy = accuracy_score(Y_train, svm.predict(X_train))\n",
    "    test_accuracy = accuracy_score(Y_test, svm.predict(X_test))\n",
    "    return num_vectors, train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4068c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to run all the models, return accuracies \n",
    "#Parameters: c1, c2, are the classes need to be tested.\n",
    "def svm_explore(X_train, X_test, Y_train, Y_test, c1, c2):\n",
    "    Y_train_indices = (Y_train== c1) | (Y_train==c2)\n",
    "    Y_test_indices = (Y_test==c1)| (Y_test==c2)\n",
    "    X_train_filtered = X_train[Y_train_indices]\n",
    "    X_test_filtered = X_test[Y_test_indices]\n",
    "    Y_train_filtered = Y_train[Y_train_indices]\n",
    "    Y_test_filtered = Y_test[Y_test_indices]\n",
    "    results = []\n",
    "    for i in range(5,51,5):\n",
    "        Num_vectors, Train_accuracy, Test_accuracy = svm_linear(X_train= X_train_filtered, X_test=X_test_filtered, Y_train=Y_train_filtered, Y_test=Y_test_filtered,c=i)\n",
    "        results.append((f\"Linear/C={i}\",Num_vectors, Train_accuracy, Test_accuracy))\n",
    "    for i in range(2,6):\n",
    "        Num_Vectors, Train_accuracy, Test_accuracy = svm_polynomial(X_train= X_train_filtered, X_test=X_test_filtered, Y_train=Y_train_filtered, Y_test=Y_test_filtered, d = i,c=10)\n",
    "        results.append((f\"Poly/d={i}\",Num_vectors, Train_accuracy, Test_accuracy))\n",
    "    for i in range(2,10,2):\n",
    "        Num_vectors, Train_accuracy, Test_accuracy = svm_rbf(X_train= X_train_filtered, X_test=X_test_filtered, Y_train=Y_train_filtered, Y_test=Y_test_filtered, sigma = i)\n",
    "        results.append((f\"RBF/sigma={i}\",Num_vectors, Train_accuracy, Test_accuracy))\n",
    "    return pd.DataFrame(results, columns = ['Name', 'Number of Vectors','Train Accuracy', 'Test Accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a74b6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1= 'A'\n",
    "c2 = 'Z'\n",
    "res = svm_explore(X_train, X_test, Y_train, Y_test, c1, c2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f919158b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890650d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data for plotting\n",
    "linear_data = res.iloc[:10]\n",
    "poly_data = res.iloc[10:14]\n",
    "rbf_data = res.iloc[14:]\n",
    "\n",
    "# Plotting training and test accuracies vs. complexity parameters for Linear SVM\n",
    "plt.figure(figsize=(12, 20))\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(range(len(linear_data)), linear_data['Train Accuracy'], marker='o', label='Train Accuracy')\n",
    "plt.plot(range(len(linear_data)), linear_data['Test Accuracy'], marker='o', label='Test Accuracy')\n",
    "plt.xticks(range(len(linear_data)), linear_data['Name'], rotation=45)\n",
    "plt.xlabel('Linear SVM Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Linear SVM Accuracy vs. Complexity Parameters')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and test accuracies vs. complexity parameters for Polynomial SVM\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(range(len(poly_data)), poly_data['Train Accuracy'], marker='o', label='Train Accuracy')\n",
    "plt.plot(range(len(poly_data)), poly_data['Test Accuracy'], marker='o', label='Test Accuracy')\n",
    "plt.xticks(range(len(poly_data)), poly_data['Name'], rotation=45)\n",
    "plt.xlabel('Polynomial SVM Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Polynomial SVM Accuracy vs. Complexity Parameters')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting training and test accuracies vs. complexity parameters for RBF SVM\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(range(len(rbf_data)), rbf_data['Train Accuracy'], marker='o', label='Train Accuracy')\n",
    "plt.plot(range(len(rbf_data)), rbf_data['Test Accuracy'], marker='o', label='Test Accuracy')\n",
    "plt.xticks(range(len(rbf_data)), rbf_data['Name'], rotation=45)\n",
    "plt.xlabel('RBF SVM Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('RBF SVM Accuracy vs. Complexity Parameters')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b39c94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scatter plot of Support vectors vs Training Accuracy\n",
    "scatter = res.iloc[:9]\n",
    "plt.scatter(res.iloc[:,1],res.iloc[:,3])\n",
    "plt.xlabel('Accuracy')\n",
    "plt.ylabel('Number of support Vectors')\n",
    "plt.title('Number of support Vectors Vs Training Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75db7c",
   "metadata": {},
   "source": [
    "Inferences:\n",
    "1. In the Linear model SVM, the model is robust to changes in the hyperparameter, with consistent high accuracies and a constant number of support vectors.\n",
    "2. In the Polynomial model SVM, increasing the polynomial degree leads to decreased accuracies, potentially indicating overfitting, while the number of support vectors remains constant.\n",
    "3. In the RBF model SVM, increasing the sigma value generally leads to improved accuracies, and the number of support vectors varies, indicating changes in the complexity of the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34948d02",
   "metadata": {},
   "source": [
    "# PROBLEM 3  Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a5f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to call random forrest and run it for different hyperparameters:\n",
    "#Parameters: c1, c2 classes need to be tested.\n",
    "def random_forrest_explore(X_train, Y_train, Y_test, X_test, c1, c2):\n",
    "    Y_train_indices = (Y_train==c1)|(Y_train==c2)\n",
    "    Y_test_indices = (Y_test==c1)|(Y_test==c2)\n",
    "    X_train_filtered = X_train[Y_train_indices]\n",
    "    X_test_filtered = X_test[Y_test_indices]\n",
    "    Y_train_filtered = Y_train[Y_train_indices]\n",
    "    Y_test_filtered = Y_test[Y_test_indices]\n",
    "    results = []\n",
    "    for i in range(5,101,5): #Hyperparameter loop to increase the trees.\n",
    "        for j in range(3,10,1): #To change the depths\n",
    "            randomForest = RandomForestClassifier(n_estimators=i,max_depth=j)\n",
    "            randomForest.fit(X_train_filtered,Y_train_filtered)\n",
    "            train_accuracy = accuracy_score(Y_train_filtered, randomForest.predict(X_train_filtered))\n",
    "            test_accuracy = accuracy_score(Y_test_filtered, randomForest.predict(X_test_filtered))\n",
    "            results.append((i,j, train_accuracy, test_accuracy))\n",
    "    return pd.DataFrame(results, columns=['Number of Trees','Depth','Train Accuracy', 'Test Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd890680",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3 = random_forrest_explore(X_train, Y_train, Y_test, X_test, c1='A', c2='Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476693f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c52fabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "res3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e4df66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Depth on the x-axis with one line for training and one for test for different numbers of trees\n",
    "plt.figure(figsize=(15, 10))\n",
    "for n_trees, group in res3.groupby('Number of Trees'):\n",
    "    plt.plot(group['Depth'], group['Train Accuracy'], label=f'Train (n_trees={n_trees})')\n",
    "    plt.plot(group['Depth'], group['Test Accuracy'], label=f'Test (n_trees={n_trees})')\n",
    "plt.xlabel('Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Depth')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5), fontsize='large')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Number of trees on the x-axis with one line for training and one for test for different depths\n",
    "plt.figure(figsize=(12, 6))\n",
    "for depth, group in res3.groupby('Depth'):\n",
    "    plt.plot(group['Number of Trees'], group['Train Accuracy'], label=f'Train (depth={depth})')\n",
    "    plt.plot(group['Number of Trees'], group['Test Accuracy'], label=f'Test (depth={depth})')\n",
    "plt.xlabel('Number of Trees')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs Number of Trees')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b91d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting using plotly expresses just to get labels on hovering\n",
    "fig1 = px.line(res3, x='Depth', y=['Train Accuracy', 'Test Accuracy'], color='Number of Trees',\n",
    "               labels={'value': 'Accuracy', 'Depth': 'Depth', 'Number of Trees': 'Number of Trees'},\n",
    "               title='Accuracy vs Depth')\n",
    "fig2 = px.line(res3, x='Number of Trees', y=['Train Accuracy', 'Test Accuracy'], color='Depth',\n",
    "               labels={'value': 'Accuracy', 'Number of Trees': 'Number of Trees', 'Depth': 'Depth'},\n",
    "               title='Accuracy vs Number of Trees')\n",
    "\n",
    "fig1.show()\n",
    "fig2.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d21d5c",
   "metadata": {},
   "source": [
    "1. Increasing Depth Effect:\n",
    "    As the depth of trees increases, both training and test accuracies tend to improve.\n",
    "    This indicates that deeper trees can capture more intricate patterns in the training data, leading to better performance on     both training and test sets.\n",
    "2. Increasing Number of Trees Effect:\n",
    "    Similarly, increasing the number of trees generally results in higher accuracies for both training and test sets.\n",
    "    This suggests that having more trees in the forest improves the model's ability to generalize to unseen data.\n",
    "3. Tradeoff Analysis:\n",
    "    Deeper trees (higher depth) can lead to overfitting, where the model learns to capture noise in the training data rather       than generalizable patterns. \n",
    "    Increasing the number of trees helps reduce overfitting by averaging predictions from multiple trees, leading to better         generalization performance.\n",
    "4. Optimal Parameters:\n",
    "    it seems that moderate depths and a larger number of trees lead to better generalization performance.\n",
    "\n",
    "Summary:\n",
    "\n",
    "From the obtained accuracies for all the models, the tradeoff between the number of trees and the depth of trees in Random Forrest involves balancing model complexity and generalization performance. Increasing the number of trees generally improves generalization, while increasing the depth of trees can lead to higher accuracy on the training set but may risk overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8634ce44",
   "metadata": {},
   "source": [
    "# PROBLEM 4  Pair-wise Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4398fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fisher discriminant function to calculate fisher scores:\n",
    "def fisher_discriminant(X, y):\n",
    "    class_means = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_means.append(np.mean(X[y == class_label], axis=0))\n",
    "    \n",
    "    overall_mean = np.mean(X, axis=0)\n",
    "    \n",
    "    S_within = np.zeros(X.shape[1])\n",
    "    for class_label, class_mean in zip(np.unique(y), class_means):\n",
    "        class_data = X[y == class_label]\n",
    "        diff = class_data - class_mean\n",
    "        S_within += np.sum(diff ** 2, axis=0)\n",
    "    \n",
    "    S_between = np.zeros(X.shape[1])\n",
    "    for class_label, class_mean in zip(np.unique(y), class_means):\n",
    "        n = np.sum(y == class_label)\n",
    "        diff = (class_mean - overall_mean) ** 2\n",
    "        S_between += n * diff\n",
    "    \n",
    "    fisher_values = np.abs(S_between / S_within)\n",
    "    return fisher_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8555032",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d088381",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all the unique classes in the dataset.\n",
    "unique_classes = sorted(data['labels'].unique())\n",
    "class_pairs = list(combinations(unique_classes, 2))\n",
    "len(class_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff413513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store results in the results list\n",
    "results = []\n",
    "\n",
    "#Iterate over pairs and calculate fisher score and SVM.\n",
    "for class_pair in class_pairs:\n",
    "    class_1, class_2 = class_pair\n",
    "    \n",
    "    # Extract data for the current class pair\n",
    "    X_train_pair = X_train[(Y_train == class_1) | (Y_train == class_2)]\n",
    "    y_train_pair = Y_train[(Y_train == class_1) | (Y_train == class_2)]\n",
    "    \n",
    "    X_val_pair = X_test[(Y_test == class_1) | (Y_test == class_2)]\n",
    "    y_val_pair = Y_test[(Y_test == class_1) | (Y_test == class_2)]\n",
    "    \n",
    "    # Calculate Fisher discriminant for the current pair\n",
    "    fisher_values = fisher_discriminant(X_train_pair, y_train_pair)\n",
    "    \n",
    "    # Select top 30 Fisher dimensions with positive values\n",
    "    positive_fisher_indices = np.where(fisher_values > 0)[0]\n",
    "    top_fisher_indices = np.argsort(fisher_values[positive_fisher_indices])[::-1][:30]\n",
    "    \n",
    "    # Map the indices back to the original indices\n",
    "    top_fisher_indices = positive_fisher_indices[top_fisher_indices]\n",
    "    \n",
    "    # Train Linear SVM\n",
    "    svm = SVC(kernel='linear', C=1.0)\n",
    "    svm.fit(X_train_pair.iloc[:, top_fisher_indices], y_train_pair)\n",
    "    y_pred = svm.predict(X_val_pair.iloc[:, top_fisher_indices])\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val_pair, y_pred)\n",
    "    print(accuracy)\n",
    "    # Append results\n",
    "    results.append((class_1, class_2, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result4 = pd.DataFrame(results, columns = ['class-1','class-2','Validation Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c4db66",
   "metadata": {},
   "outputs": [],
   "source": [
    "result4['pairs'] = result4['class-1'] + '-' + result4['class-2']\n",
    "result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a27f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top features in the data set\n",
    "top_fisher_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34eec414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion to categorize diffcult vs easy:\n",
    "#Parameters: Validationa accuracies. threshold can be any \n",
    "def categorize_difficulty(validation_accuracies, threshold=0.95):\n",
    "    difficult_pairs = []\n",
    "    easy_pairs = []\n",
    "    \n",
    "    for i, accuracy in enumerate(validation_accuracies):\n",
    "        if accuracy < threshold:\n",
    "            difficult_pairs.append(i)\n",
    "        else:\n",
    "            easy_pairs.append(i)\n",
    "    \n",
    "    return difficult_pairs, easy_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30da57f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the diffcult and easy pairs\n",
    "validation_accuracies=result4['Validation Accuracy']\n",
    "difficult_pairs, easy_pairs = categorize_difficulty(validation_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945335a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "difficult_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cbe3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acce0a3",
   "metadata": {},
   "source": [
    "1. Pairwise Comparison:\n",
    "   The results allows for a pairwise comparison of validation accuracies between different pairs of classes.\n",
    "   This pairwise comparison provides insights into how well a classifier performs when distinguishing between each pair of        classes.\n",
    "\n",
    "2. Variation in Accuracies:\n",
    "    The validation accuracies vary across different pairs of classes, ranging from 0.975 to 0.993.\n",
    "    This variation suggests that the difficulty of distinguishing between different pairs of classes may vary, with some pairs     being easier to classify than others.\n",
    "    \n",
    "3. Identification of Challenging Class Pairs:\n",
    "    Pairs with lower validation accuracies may indicate more challenging class distinctions for the classifier.\n",
    "\n",
    "Summary:\n",
    "       \n",
    "  The provided table offers insights into the classifier's performance when distinguishing between different pairs of classes. It highlights variations in classification difficulty across class pairs and provides valuable information for understanding and potentially improving the classifier's overall performance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
